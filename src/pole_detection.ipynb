{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road-Pole/Stick Detection on Edge Devices\n",
    "\n",
    "a research paper by Sindre L. Ã˜yen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 Introduction\n",
    "In this paper, I will investigate the feasibility of using edge devices for pole detection, applying modern solutions for Visual Intelligence (VI). First, the data preparation phase will be explained, where the data set is visualized and discussed. Second, the model selection phase will be documented, discussing available options weighing them against one another based on performance and accuracy. Third, a model will be trained on the dataset and monitored based on performance metrics. Hence, three final sections will document (1) the testing and evaluation phase based on relevant metrics, (2) the deployment feasibility of the model on e.g., an edge device, and (3) the sustainability of the training and inference of the developed model will be discussed with examples and comparisons of energy consumption.\n",
    "\n",
    "Road poles, commonly referred to as \"brÃ¸ytestikker\" in Norwegian, are central in navigation guidance during the winter - especially for giving guidance to plowers as to where the road edge is positioned on overly snowy roads. Recently, in the realm of Cooperative, Connected and Automated Mobility (CCAM) (i.e., self-driving vehicles) research, studies investigate the feasibility of using VI technology to detect road poles and more efficiently navigate Norwegian roads under diverse conditions. Thus, road poles may have an ever-growing importance in the Norwegian infrastructure and the dataset collection which on the larger scale is the motivation of this research paper.\n",
    "\n",
    "The end goal is to produce an accurate model that has the potential to be ran with real-time inference on a hardware-restricted edge-device, such as a smartphone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NAPLab LiDAR Image Dataset\n",
    "\n",
    "NAPLab has collected and annotated a dataset of LiDAR images of poles which has been made available for this project. As mentioned in the repository README, this dataset is unavailable for redistribution. However, with access to the NTNU Idun cluster the dataset is made found under `/cluster/projects/vc/data/ad/open/Poles`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Discussion of the NAPLab Dataset\n",
    "\n",
    "The dataset contains annotated data with one class as the basis for detection, namely `pole`, which refers to a road pole. While writing this paper, the dataset comprises a total of 2 259 images, divided into a train and a validation split of 1 809 and 450 images, respectively. In the figure below, an example image from the dataset is linked. The dataset has a total of 4 230 annotations, with an average of 1.9 annotations per image; indicating the expected result that road poles come in pairs, with some exceptions. The images are of median image ratio `1024 x 128` and are thus of an ultra-wide format. The training dataset contains 3 378 annotations and the validation dataset contains 852. Currently, no test set is present.\n",
    "\n",
    "<img src=\"assets/roadpoles_example.png\" width=\"1000\"/>\n",
    "\n",
    "###### Â© NAPLab \n",
    "\n",
    "Notably, as aforementioned and as visible in the example, the dataset is based on LiDAR images of poles. On one side, this reduces the feasibility of a potential result being used on e.g., a smartphone and increments the hardware cost for real-time inference. Modern smartphones such as the newer iPhone Pro and Pro Max models do have LiDAR sensors but with a limited reach of around 5 meters On the other side, LiDAR images, while being costly, are more available in the varying lighting conditions found in Norway. Nonetheless, the exploration in this project will be based on the LiDAR images in this dataset, evaluated on the potential performance on a hypothetical hardware-restrained edge device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 The Train/Val Split in this Paper\n",
    "\n",
    "Seeing that the dataset does not comprise a test dataset, the training dataset will be utilized to create a new train/validation split. The original validation dataset will be used for comparing the end result to other solutions, effectively acting as the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Bibtex Reference to the Dataset\n",
    "\n",
    "```biblatex\n",
    "    @misc{\n",
    "        polpolpol-pol-det_dataset,\n",
    "        title = { polpolpol pol det Dataset },\n",
    "        type = { Open Source Dataset },\n",
    "        author = { DURGA PRASAD },\n",
    "        howpublished = { \\url{ https://universe.roboflow.com/durga-prasad-hmjq7/polpolpol-pol-det } },\n",
    "        url = { https://universe.roboflow.com/durga-prasad-hmjq7/polpolpol-pol-det },\n",
    "        journal = { Roboflow Universe },\n",
    "        publisher = { Roboflow },\n",
    "        year = { 2024 },\n",
    "        month = { oct },\n",
    "        note = { visited on 2024-11-23 },\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the end goal is to produce an accurate model that can be performant for real-time inference on a hardware-restricted edge device, the goal is to use a lightweight model. In my previous exploratory study in the TDT17 Visual Intelligence course, I stipulated the following comparative overview of performance metrics of lightweight Object Detection (OD) models:\n",
    "\n",
    "<img src=\"assets/yolo-comparison.png\" alt=\"A comparison between YOLO models and EfficientDet\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Overview by Sindre Ã˜yen, based on data from Ultralytics and EfficientDet (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above highlights the performance of different real-time Object Detection (OD) models on the Microsoft Common Objects in Context (COCO) dataset, which often serves as a benchmark for evaluating OD models. EfficientDet-D1 has low computational requirements while maintaining reasonable accuracy, with a reasonable precision-to-computation for edge-device deployment. However, its accuracy (mAP) is only slightly better than the smallest YOLO model, YOLO11n. For tasks requiring higher precision, EfficientDet models have rapidly increasing computational needs for the larger models (e.g., EfficientDet-D2, D3), effectively making EfficientDet less ideal for edge-device deployment.\n",
    "\n",
    "On the other side, the YOLO11 series has a slower increase in computational demands for increased model size. Among the lightweight models, YOLO11s represents the best precision-to-computation trade-off - with superior precision to e.g., the EfficientDet-D3 model with 14% less floating point operations per second (FLOPs). Moreover, the YOLO11 series is specifically designed for real-time usage, with optimized inference speeds that are essential for edge deployment. YOLO11s, in particular, has both a compact model architecture and a solid detection performance and is thus a good starting point for this project, focusing on achieving reliable, real-time pole detection on edge-devices.\n",
    "\n",
    "In summary, the hypothesis is that YOLO11s will deliver the best precision-to-efficiency ratio for this use case, emphasizing effective detection of road poles while also considering compatibility with the computational limits of edge-devices. Further exploration may refine this initial choice of model, but YOLO11s proves a strong foundation based on benchmarked results and architectural requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train / Val Split\n",
    "\n",
    "As mentioned earlier in the paper, the dataset does not contain test data - only a training / validation split. To support testing for comparing with other solutions, I will be creating a new training / validation split from the training dataset, leaving the current validation dataset untouched to be used for training. \n",
    "\n",
    "Since the project will be focusing on YOLO11 models, I have downloaded the dataset with YOLO11 supported annotations. To reproduce, download YOLO11 data and place it under `data/PolesYOLO11`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, I am referencing the folders containing the data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data folder\n",
    "__poles_dataset_name = \"PolesYOLO11\"\n",
    "__base_dataset_path = os.path.join(\"..\", \"data\", __poles_dataset_name)\n",
    "\n",
    "# Current train/val\n",
    "current_train = os.path.join(__base_dataset_path, \"train\")\n",
    "current_val = os.path.join(__base_dataset_path, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, a new dataset train/val/test split must also be created. The code block below generates a new train/test/val split. Here, I am making the assumption that strategizing the split is not necessary since there is only one class and the images seem to be similar by nature. Moreover, I am selecting a 80/20 split between training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new dataset\n",
    "revised_dataset_path = os.path.join(\"..\", \"data\", \"RevisedPolesY11\")\n",
    "\n",
    "# New train/val\n",
    "new_train = os.path.join(revised_dataset_path, \"train\")\n",
    "new_val = os.path.join(revised_dataset_path, \"valid\")\n",
    "new_test = os.path.join(revised_dataset_path, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the new dataset folder\n",
    "os.makedirs(revised_dataset_path, exist_ok=True)\n",
    "os.makedirs(new_train, exist_ok=True)\n",
    "os.makedirs(new_val, exist_ok=True)\n",
    "os.makedirs(new_test, exist_ok=True)\n",
    "\n",
    "# Copy the files from current val to new test\n",
    "os.system(f\"cp -r {current_val}/* {new_test}/\")\n",
    "\n",
    "# Create a 80/20 split from the current train to the new train and new val\n",
    "current_train_images = os.path.join(current_train, \"images\")\n",
    "current_train_labels = os.path.join(current_train, \"labels\")\n",
    "\n",
    "# Get the list of images\n",
    "images = os.listdir(current_train_images)\n",
    "\n",
    "# Split the images\n",
    "split = int(len(images) * 0.8)\n",
    "train_images = images[:split]\n",
    "val_images = images[split:]\n",
    "\n",
    "# Copy the images\n",
    "for image in train_images:\n",
    "    os.system(f\"cp {os.path.join(current_train_images, image)} {os.path.join(new_train, 'images', image)}\")\n",
    "    os.system(f\"cp {os.path.join(current_train_labels, image.replace('.jpg', '.txt'))} {os.path.join(new_train, 'labels', image.replace('.jpg', '.txt'))}\")\n",
    "\n",
    "for image in val_images:\n",
    "    os.system(f\"cp {os.path.join(current_train_images, image)} {os.path.join(new_val, 'images', image)}\")\n",
    "    os.system(f\"cp {os.path.join(current_train_labels, image.replace('.jpg', '.txt'))} {os.path.join(new_val, 'labels', image.replace('.jpg', '.txt'))}\")\n",
    "\n",
    "print(\"New train/val/test dataset created!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ultralytics [python package](https://pypi.org/project/ultralytics/) is used to train the model. This code is extracted into `train_model.py` to ensure readability for the notebook. However, the reflections around the choices and the results will still be documented in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is straight forward and is thoroughly documented in the [ultralytics training docs](https://docs.ultralytics.com/modes/train/). For more specialized training, ultralytics also supports data augmentation. Drawing inspiration from [0] where data augmentations s.a., shear, grayscale conversion, and adjustments in hue, saturation, brightness, was used to strengthen the model's robustness - this training also uses data augmentation to improve the final result. This augmentation was implemented with the following initial reflections:\n",
    "\n",
    "- The lidar images are not grayscale, meaning photometric transformations like hue, saturation, and brightness adjustments are relevant for simulating diverse environmental conditions.\n",
    "- Poles have a consistent geometric structure, so geometric augmentations like small rotations and translations may enhance robustness without introducing unrealistic distortions.\n",
    "- Mosaic augmentation can add contextual richness, which can be valuable for single-class datasets, and label smoothing regularizes the model to further prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmentations that were selected are argued for, and discussed, in the table below. These were selected, based on inspirations from [0] and the initial reflections. Label smoothing\n",
    "\n",
    "| **Category**               | **Augmentation**          | **Parameter & Value** | **Explanation**                                                                                                                                                           |\n",
    "|----------------------------|--------------------------|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Geometric Transformations** | **Rotation**             | `degrees=2.0`         | Slight rotations to account for variations in camera or lidar angles. Keeps the poles' vertical orientation without introducing distortions.                            |\n",
    "|                            | **Scaling**              | `scale=0.4`           | Scales images by Â±40% to simulate poles at different distances from the lidar sensor, ensuring robustness to size variations.                                           |\n",
    "|                            | **Translation**          | `translate=0.1`       | Translates images by Â±10% to account for positional shifts caused by sensor placement or road alignment.                                                                |\n",
    "|                            | **Shearing**             | `shear=2.0`           | Minimal shearing to simulate perspective changes while preserving pole geometry.                                                                                        |\n",
    "| **Photometric Transformations** | **Hue Adjustment**       | `hsv_h=0.015`         | Adjusts hue slightly (Â±1.5%) to reflect environmental lighting changes that may alter the color properties of lidar data.                                               |\n",
    "|                            | **Saturation Adjustment** | `hsv_s=0.5`           | Modulates saturation by Â±50% to account for intensity variations caused by different surface materials or weather conditions.                                           |\n",
    "|                            | **Brightness Adjustment** | `hsv_v=0.4`           | Adjusts brightness by Â±40% to simulate reflectance intensity differences in diverse lighting conditions, such as overcast or sunny environments.                        |\n",
    "| **Advanced Augmentation**  | **Mosaic Augmentation**   | `mosaic=1.0`          | Combines multiple images into one, creating new spatial contexts and increasing variability. Useful for single-class datasets with limited positional diversity.         |\n",
    "|                            | **Label Smoothing**       | `label_smoothing=0.05`| Applies a 5% smoothing factor to prevent overconfidence in predictions, reducing overfitting in single-class datasets like this one.                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Disclaimer**: the above augmentations are a result of trial and error, as well as documentation reading. The overview layout was created with ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several models were trained in this experimental approach. All of them building on pre-trained weights downloaded from Ultralytics, as their recommendations are to train on pre-trained weights. [0] suggested grayscale transforming lidar images for model training, this yielded worse results with the current config and was thus not applied here. The results are evaluated in Section 5, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5 Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluating on the Original Valid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the models trained for this task are to be evaluated on the validation data from the original dataset. The findings presented in this section are the weights with the best accuracy from the trainings.\n",
    "\n",
    "Below, validation is ran with `.val` from the Ultralytics python package:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Running Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(path: str, data: str):\n",
    "    model = YOLO(path)\n",
    "    metrics = model.val(data=data, imgsz=1024, batch=16, iou=0.6, conf=0.25)\n",
    "\n",
    "    mAP = metrics.box.map  # mAP\n",
    "    mAP50 = metrics.box.map50  # map50\n",
    "    recall = metrics.box.r[0]  # recall\n",
    "    precision = metrics.box.p[0]  # precision\n",
    "\n",
    "    print(f\"mAP@50-95: {mAP}, \\nmAP@50: {mAP50}, \\nrecall: {recall}, \\nprecision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.36 ðŸš€ Python-3.9.18 torch-2.5.1+cu124 CUDA:0 (Tesla V100-PCIE-32GB, 32494MiB)\n",
      "YOLO11s summary (fused): 238 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /cluster/home/sindroye/tdt17-miniProject/data/PolesYOLO11/valid/labels.cache... 450 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450/450 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        450        900      0.889      0.807      0.865       0.45\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.0ms loss, 9.4ms postprocess per image\n",
      "Results saved to \u001b[1m/cluster/home/sindroye/tdt17-miniProject/runs/detect/val7\u001b[0m\n",
      "mAP@50-95: 0.4495150263177995, \n",
      "mAP@50: 0.8654423223237286, \n",
      "recall: 0.8066666666666666, \n",
      "precision: 0.8886168910648715\n"
     ]
    }
   ],
   "source": [
    "__best = os.path.join(\"..\", \"models\", \"pole_models\", f\"best\", \"weights\", \"best.pt\")\n",
    "validate(__best, \"test.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Understanding the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the original validation data, the resulting weights achieve a mAP@50-95 precision of 0.45, mAP@50 of 0.87, 0.89 in precision, and a recall rate of 0.81. The architecture of the final model consists of:\n",
    "\n",
    "- 238 layers\n",
    "- 9,41M params\n",
    "- 21.3 GFLOPs\n",
    "\n",
    "In terms of architecture, these are expected results and align with YOLO11s parameter numbers and FLOPs. The performance and precision can be better illustrated through the resulting graphs and by looking at examples. For this exploration I will first look into the graphs to interpret how the model performs and then exemplify by looking at images from actual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrices**\n",
    "\n",
    "Confusion Matrix             |  Confusion Matrix (Normalized)\n",
    ":-------------------------:|:-------------------------:\n",
    "![](assets/val/confusion_matrix.png)  |  ![](assets/val/confusion_matrix_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, after validating model performance on the original validation dataset, we learn that the model is generally good at identifying poles but have some weaknesses. Out of 900 possible road poles, the model missed 151 and accurately found 749. Additionally, the model had 60 cases of predicting a pole to exist, where none existed. The latter occurred with a rate of 0.13 times per image, whilst a pole was missed approximately every third photo. For real-time inference these results don't necessarily indicate that a pole would not be registered at all, but rather that the detection will miss detections in certain frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**\n",
    "\n",
    "F1 Curve            |  Precision-Recall Curve\n",
    ":-------------------------:|:-------------------------:\n",
    "![](assets/val/F1_curve.png)  |  ![](assets/val/PR_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Confidence      |  Recall-Confidence\n",
    ":-------------------------:|:-------------------------:\n",
    "![](assets/val/P_curve.png)  |  ![](assets/val/R_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run predictions on the test dataset, I ran the following code, here put into markdown to avoid re-running and hundreds of printed lines.\n",
    "\n",
    "```python\n",
    "# Predict with the model\n",
    "images = os.path.join(new_test, \"images\")\n",
    "results = model(images, imgsz=1024, batch=16, conf=0.25, iou=0.6)\n",
    "results_folder = os.path.join(\"..\", \"runs\", \"prediction_results\")\n",
    "\n",
    "# Process results list\n",
    "count = 1\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=os.path.join(results_folder, f\"result{count}.jpg\"))  # save to disk\n",
    "    count += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 Deployment Feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 7 Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Sustainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0]Â Bavirisetti, Durga Prasad, Gabriel Hanssen Kiss, and Frank Lindseth. â€œA Pole Detection and Geospatial Localization Framework Using LiDAR-GNSS Data Fusion.â€ FUSION 2024 - 27th International Conference on Information Fusion, 2024. https://doi.org/10.23919/FUSION59988.2024.10706275.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
