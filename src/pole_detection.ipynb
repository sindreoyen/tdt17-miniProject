{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road-Pole/Stick Detection on Edge Devices\n",
    "\n",
    "a research paper by Sindre L. Øyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 Introduction\n",
    "In this paper, I will investigate the feasibility of using edge devices for pole detection, applying modern solutions for Visual Intelligence (VI). First, the data preparation phase will be explained, where the data set is visualized and discussed. Second, the model selection phase will be documented, discussing available options weighing them against one another based on performance and accuracy. Third, a model will be trained on the dataset and monitored based on performance metrics. Hence, three final sections will document (1) the testing and evaluation phase based on relevant metrics, (2) the deployment feasibility of the model on e.g., an edge device, and (3) the sustainability of the training and inference of the developed model will be discussed with examples and comparisons of energy consumption.\n",
    "\n",
    "Road poles, commonly referred to as \"brøytestikker\" in Norwegian, are central in navigation guidance during the winter - especially for giving guidance to plowers as to where the road edge is positioned on overly snowy roads. Recently, in the realm of Cooperative, Connected and Automated Mobility (CCAM) (i.e., self-driving vehicles) research, studies investigate the feasibility of using VI technology to detect road poles and more efficiently navigate Norwegian roads under diverse conditions. Thus, road poles may have an ever-growing importance in the Norwegian infrastructure and the dataset collection which on the larger scale is the motivation of this research paper.\n",
    "\n",
    "The end goal is to produce an accurate model that has the potential to be ran with real-time inference on a hardware-restricted edge-device, such as a smartphone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NAPLab LiDAR Image Dataset\n",
    "\n",
    "NAPLab has collected and annotated a dataset of LiDAR images of poles which has been made available for this project. As mentioned in the repository README, this dataset is unavailable for redistribution. However, with access to the NTNU Idun cluster the dataset is made found under `/cluster/projects/vc/data/ad/open/Poles`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Discussion of the NAPLab Dataset\n",
    "\n",
    "The dataset contains annotated data with one class as the basis for detection, namely `pole`, which refers to a road pole. While writing this paper, the dataset comprises a total of 2 259 images, divided into a train and a validation split of 1 809 and 450 images, respectively. In the figure below, an example image from the dataset is linked. The dataset has a total of 4 230 annotations, with an average of 1.9 annotations per image; indicating the expected result that road poles come in pairs, with some exceptions. The images are of median image ratio `1024 x 128` and are thus of an ultra-wide format. The training dataset contains 3 378 annotations and the validation dataset contains 852. Currently, no test set is present.\n",
    "\n",
    "<img src=\"assets/roadpoles_example.png\" alt=\"An example of an entry in the dataset\" width=\"1000\"/>\n",
    "\n",
    "###### © NAPLab \n",
    "\n",
    "Notably, as aforementioned and as visible in the example, the dataset is based on LiDAR images of poles. On one side, this reduces the feasibility of a potential result being used on e.g., a smartphone and increments the hardware cost for real-time inference. Modern smartphones such as the newer iPhone Pro and Pro Max models do have LiDAR sensors but with a limited reach of around 5 meters On the other side, LiDAR images, while being costly, are more available in the varying lighting conditions found in Norway. Nonetheless, the exploration in this project will be based on the LiDAR images in this dataset, evaluated on the potential performance on a hypothetical hardware-restrained edge device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 The Train/Val Split in this Paper\n",
    "\n",
    "Seeing that the dataset does not comprise a test dataset, the training dataset will be utilized to create a new train/validation split. The original validation dataset will be used for comparing the end result to other solutions, effectively acting as the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Bibtex Reference to the Dataset\n",
    "\n",
    "```biblatex\n",
    "    @misc{\n",
    "        polpolpol-pol-det_dataset,\n",
    "        title = { polpolpol pol det Dataset },\n",
    "        type = { Open Source Dataset },\n",
    "        author = { DURGA PRASAD },\n",
    "        howpublished = { \\url{ https://universe.roboflow.com/durga-prasad-hmjq7/polpolpol-pol-det } },\n",
    "        url = { https://universe.roboflow.com/durga-prasad-hmjq7/polpolpol-pol-det },\n",
    "        journal = { Roboflow Universe },\n",
    "        publisher = { Roboflow },\n",
    "        year = { 2024 },\n",
    "        month = { oct },\n",
    "        note = { visited on 2024-11-23 },\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the end goal is to produce an accurate model that can be performant for real-time inference on a hardware-restricted edge device, the goal is to use a lightweight model. In my previous exploratory study in the TDT17 Visual Intelligence course, I stipulated the following comparative overview of performance metrics of lightweight Object Detection (OD) models:\n",
    "\n",
    "<img src=\"assets/yolo-comparison.png\" alt=\"A comparison between YOLO models and EfficientDet\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Overview by Sindre Øyen, based on data from Ultralytics and EfficientDet (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above highlights the performance of different real-time Object Detection (OD) models on the Microsoft Common Objects in Context (COCO) dataset, which often serves as a benchmark for evaluating OD models. EfficientDet-D1 has low computational requirements while maintaining reasonable accuracy, with a reasonable precision-to-computation for edge-device deployment. However, its accuracy (mAP) is only slightly better than the smallest YOLO model, YOLO11n. For tasks requiring higher precision, EfficientDet models have rapidly increasing computational needs for the larger models (e.g., EfficientDet-D2, D3), effectively making EfficientDet less ideal for edge-device deployment.\n",
    "\n",
    "On the other side, the YOLO11 series has a slower increase in computational demands for increased model size. Among the lightweight models, YOLO11s represents the best precision-to-computation trade-off - with superior precision to e.g., the EfficientDet-D3 model with 14% less floating point operations per second (FLOPs). Moreover, the YOLO11 series is specifically designed for real-time usage, with optimized inference speeds that are essential for edge deployment. YOLO11s, in particular, has both a compact model architecture and a solid detection performance and is thus a good starting point for this project, focusing on achieving reliable, real-time pole detection on edge-devices.\n",
    "\n",
    "In summary, the hypothesis is that YOLO11s will deliver the best precision-to-efficiency ratio for this use case, emphasizing effective detection of road poles while also considering compatibility with the computational limits of edge-devices. Further exploration may refine this initial choice of model, but YOLO11s proves a strong foundation based on benchmarked results and architectural requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train / Val Split\n",
    "\n",
    "As mentioned earlier in the paper, the dataset does not contain test data - only a training / validation split. To support testing for comparing with other solutions, I will be creating a new training / validation split from the training dataset, leaving the current validation dataset untouched to be used for training. \n",
    "\n",
    "Since the project will be focusing on YOLO11 models, I have downloaded the dataset with YOLO11 supported annotations. To reproduce, download YOLO11 data and place it under `data/PolesYOLO11`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, I am referencing the folders containing the data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data folder\n",
    "__poles_dataset_name = \"PolesYOLO11\"\n",
    "__base_dataset_path = os.path.join(\"..\", \"data\", __poles_dataset_name)\n",
    "\n",
    "# Current train/val\n",
    "current_train = os.path.join(__base_dataset_path, \"train\")\n",
    "current_val = os.path.join(__base_dataset_path, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, a new dataset train/val/test split must also be created. The code block below generates a new train/test/val split. Here, I am making the assumption that strategizing the split is not necessary since there is only one class and the images seem to be similar by nature. Moreover, I am selecting a 80/20 split between training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train/val/test dataset created!\n"
     ]
    }
   ],
   "source": [
    "# Path to the new dataset\n",
    "revised_dataset_path = os.path.join(\"..\", \"data\", \"RevisedPolesY11\")\n",
    "\n",
    "# New train/val\n",
    "new_train = os.path.join(revised_dataset_path, \"train\")\n",
    "new_val = os.path.join(revised_dataset_path, \"valid\")\n",
    "new_test = os.path.join(revised_dataset_path, \"test\")\n",
    "\n",
    "# Create the new dataset folder\n",
    "os.makedirs(revised_dataset_path, exist_ok=True)\n",
    "os.makedirs(new_train, exist_ok=True)\n",
    "os.makedirs(new_val, exist_ok=True)\n",
    "os.makedirs(new_test, exist_ok=True)\n",
    "\n",
    "# Copy the files from current val to new test\n",
    "os.system(f\"cp -r {current_val}/* {new_test}/\")\n",
    "\n",
    "# Create a 80/20 split from the current train to the new train and new val\n",
    "current_train_images = os.path.join(current_train, \"images\")\n",
    "current_train_labels = os.path.join(current_train, \"labels\")\n",
    "\n",
    "# Get the list of images\n",
    "images = os.listdir(current_train_images)\n",
    "\n",
    "# Split the images\n",
    "split = int(len(images) * 0.8)\n",
    "train_images = images[:split]\n",
    "val_images = images[split:]\n",
    "\n",
    "# Copy the images\n",
    "for image in train_images:\n",
    "    os.system(f\"cp {os.path.join(current_train_images, image)} {os.path.join(new_train, 'images', image)}\")\n",
    "    os.system(f\"cp {os.path.join(current_train_labels, image.replace('.jpg', '.txt'))} {os.path.join(new_train, 'labels', image.replace('.jpg', '.txt'))}\")\n",
    "\n",
    "for image in val_images:\n",
    "    os.system(f\"cp {os.path.join(current_train_images, image)} {os.path.join(new_val, 'images', image)}\")\n",
    "    os.system(f\"cp {os.path.join(current_train_labels, image.replace('.jpg', '.txt'))} {os.path.join(new_val, 'labels', image.replace('.jpg', '.txt'))}\")\n",
    "\n",
    "print(\"New train/val/test dataset created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ultralytics [python package](https://pypi.org/project/ultralytics/) is used to train the model. This code is extracted into `train_model.py` to ensure readability for the notebook. However, the reflections around the choices and the results will still be documented in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is straight forward and is thoroughly documented in the [ultralytics trainind docs](https://docs.ultralytics.com/modes/train/). For more specialized training, ultralytics also supports data augmentation. Data augmentation parameters available in the `.train()` method for the ultralytics package are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5 Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 Methods / Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 7 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8.2 Key Learning Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
